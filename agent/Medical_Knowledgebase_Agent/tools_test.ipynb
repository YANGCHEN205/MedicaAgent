{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import tools_map, query_user_for_details,gen_tools_desc\n",
    "from prompt import gen_prompt, user_prompt\n",
    "tools_dict= gen_tools_desc()\n",
    "print(tools_dict)\n",
    "print(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import tools_map, query_user_for_details\n",
    "from prompt import gen_prompt, user_prompt\n",
    "from model_provider import ModelProvider\n",
    "from dotenv import load_dotenv\n",
    "import dashscope\n",
    "import os\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class Medical_Knowledgebase_Agent:\n",
    "    def __init__(self):\n",
    "        self.mp = ModelProvider()\n",
    "        dashscope.api_key = \"sk-f529539e3a50472fac783cf1e99fddef\"\n",
    "        os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "\n",
    "    def parse_thoughts(self, response, cur_request_time, max_request_time, debug):\n",
    "        try:\n",
    "            thoughts = response.get(\"thoughts\")\n",
    "            planning = thoughts.get(\"planning\")\n",
    "            reasoning = thoughts.get(\"reasoning\")\n",
    "            reflection = thoughts.get(\"reflection\")\n",
    "            history = thoughts.get(\"history\")\n",
    "            summary = thoughts.get(\"summary\")\n",
    "            observation = response.get(\"observation\")\n",
    "            prompt = f\"planning: {planning}\\nreasoning: {reasoning}\\nreflection: {reflection}\\nhistory: {history}\\nobservation: {observation}\\nsummary: {summary}\"\n",
    "            prompt += f\"\\n这是医疗知识库第{cur_request_time}次响应，最多执行{max_request_time}次\\n\"\n",
    "            if debug:\n",
    "                print(prompt)\n",
    "            return prompt\n",
    "        except Exception as e:\n",
    "            print(f\"parse_thoughts error: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def execute_action(self, action_name, action_args, debug):\n",
    "        try:\n",
    "            func = tools_map.get(action_name)\n",
    "            result = func(**action_args)\n",
    "            if debug:\n",
    "                print(f\"action_name: {action_name}, action_args: {action_args}\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"调用工具异常： {e}\")\n",
    "            return str(e)\n",
    "\n",
    "    def agent_execute(self, query, max_request_time=5, debug=False):\n",
    "        cur_request_time = 0\n",
    "        chat_history = []\n",
    "        agent_scratch = \"\"\n",
    "\n",
    "        while cur_request_time < max_request_time:\n",
    "            cur_request_time += 1\n",
    "            prompt = gen_prompt(query, agent_scratch)\n",
    "            response = self.mp.chat(prompt, chat_history)\n",
    "\n",
    "            if not response or not isinstance(response, dict):\n",
    "                print(f\"call llm exception, response is: {response}\")\n",
    "                continue\n",
    "\n",
    "            action_info = response.get(\"action\")\n",
    "            action_name = action_info.get(\"name\")\n",
    "            action_args = action_info.get(\"parameters\")\n",
    "            if debug:\n",
    "                print(f'-------------医学知识库Agent第{cur_request_time}次推断------------')\n",
    "                print(response)\n",
    "                print(json.dumps(response, ensure_ascii=False, indent=4))\n",
    "\n",
    "            if action_name == \"query_user_for_details\":\n",
    "                user_response = input(query_user_for_details(action_args[\"prompt\"]))\n",
    "                chat_history.append([action_args[\"prompt\"], user_response])\n",
    "                agent_scratch += f\"query_user: {action_args['prompt']}user response: {user_response}\"\n",
    "                continue\n",
    "\n",
    "            call_function_result = self.execute_action(action_name, action_args, debug)\n",
    "            agent_scratch += f\"observation: {response.get('observation')} execute action {action_name} result: {call_function_result}\"\n",
    "            assistant_msg = self.parse_thoughts(response, cur_request_time, max_request_time, debug)\n",
    "            chat_history.append([user_prompt, assistant_msg])\n",
    "\n",
    "            if action_name == \"finish\":\n",
    "                final_answer = action_args.get(\"answer\")\n",
    "                break\n",
    "\n",
    "        if cur_request_time == max_request_time:\n",
    "            final_answer = \"本次任务执行失败！未能提供相关医学知识\"\n",
    "        print(f\"final_answer: {final_answer}\")\n",
    "        return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用类\n",
    "agent = Medical_Knowledgebase_Agent()\n",
    "max_request_time = 6\n",
    "query = '你好，你能告诉我今天星期几吗？'\n",
    "final_answer = agent.agent_execute(query, max_request_time=max_request_time,debug=True)\n",
    "print(f\"final_answer: {final_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tools import tools_map, query_user_for_details\n",
    "# from prompt import gen_prompt, user_prompt\n",
    "# from model_provider import ModelProvider\n",
    "# from dotenv import load_dotenv\n",
    "# import dashscope\n",
    "# import os\n",
    "# load_dotenv()\n",
    "# mp = ModelProvider()\n",
    "\n",
    "# dashscope.api_key = 'sk-f529539e3a50472fac783cf1e99fddef'\n",
    "# os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "# # 解析模型返回的响应，提取关键信息并格式化为字符串\n",
    "# def parse_thoughts(response):\n",
    "#     try:\n",
    "#         thoughts = response.get(\"thoughts\")\n",
    "#         observation = response.get(\"observation\")\n",
    "#         planning = thoughts.get(\"planning\")\n",
    "#         reasoning = thoughts.get(\"reasoning\")\n",
    "#         reflection = thoughts.get(\"reflection\")\n",
    "#         summary = thoughts.get(\"summary\")\n",
    "#         query_user = thoughts.get(\"query_user\")\n",
    "#         prompt = f\"planning: {planning}reasoning: {reasoning}reflection: {reflection}observation: {observation}summary: {summary}query_user: {query_user}\"\n",
    "#         return prompt\n",
    "#     except Exception as e:\n",
    "#         print(f\"parse_thoughts error: {e}\")\n",
    "#         return \"\"\n",
    "#     return {}\n",
    "\n",
    "# # 执行代理任务，与模型交互并处理结果\n",
    "# def agent_execute(query, max_request_time):\n",
    "#     cur_request_time = 0\n",
    "#     chat_history = []\n",
    "#     agent_scratch = \"\"\n",
    "#     while cur_request_time < max_request_time:\n",
    "#         cur_request_time += 1\n",
    "#         prompt = gen_prompt(query, agent_scratch)\n",
    "#         print('开始调用通义千问.....')\n",
    "#         response = mp.chat(prompt, chat_history)\n",
    "#         print(response)\n",
    "#         if not response or not isinstance(response, dict):\n",
    "#             print(f\"call llm exception, response is: {response}\")\n",
    "#             continue\n",
    "#         action_info = response.get(\"action\")\n",
    "#         action_name = action_info.get(\"name\")\n",
    "#         action_args = action_info.get(\"args\")\n",
    "#         print(f\"action_name: {action_name}, action_args: {action_args}\")\n",
    "#         # thoughts = response.get(\"thoughts\")\n",
    "#         # planning = thoughts.get(\"planning\")\n",
    "#         # reasoning = thoughts.get(\"reasoning\")\n",
    "#         # reflection = thoughts.get(\"reflection\")\n",
    "#         # summary = thoughts.get(\"summary\")\n",
    "#         # observation = response.get(\"observation\")\n",
    "#         # print(f\"observation: {observation}\")\n",
    "#         # print(f\"planning: {planning}\")\n",
    "#         # print(f\"reasoning: {reasoning}\")\n",
    "#         # print(f\"reflection: {reflection}\")\n",
    "#         # print(f\"summary: {summary}\")\n",
    "#         if action_name == \"query_user_for_details\":\n",
    "#             user_response = input(query_user_for_details(action_args['prompt']))\n",
    "#             chat_history.append([query_user_for_details(action_args['prompt']), user_response])\n",
    "#             agent_scratch = agent_scratch + f\"query_user: {query_user_for_details(action_args['prompt'])}user response: {user_response}\"\n",
    "#             continue\n",
    "#         try:\n",
    "#             func = tools_map.get(action_name)\n",
    "#             call_function_result = func(**action_args)\n",
    "#         except Exception as e:\n",
    "#             print(f\"调用工具异常： {e}\")\n",
    "#             call_function_result = f\"{e}\"\n",
    "#         agent_scratch = agent_scratch + f\"observation: {observation}execute action result: {call_function_result}\"\n",
    "#         assistant_msg = parse_thoughts(response)\n",
    "#         chat_history.append([user_prompt, assistant_msg])\n",
    "#         if action_name == \"finish\":\n",
    "#             final_answer = action_args.get(\"answer\")\n",
    "#             print(f\"final_answer: {final_answer}\")\n",
    "#             break\n",
    "#         observation = response.get(\"observation\")\n",
    "#     if cur_request_time == max_request_time:\n",
    "#         print(\"本次任务执行失败！\")\n",
    "#     else:\n",
    "#         print(\"本次任务成功！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import tools_map, query_user_for_details\n",
    "from prompt import gen_prompt, user_prompt\n",
    "from model_provider import ModelProvider\n",
    "from dotenv import load_dotenv\n",
    "import dashscope\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "mp = ModelProvider()\n",
    "\n",
    "dashscope.api_key = \"sk-f529539e3a50472fac783cf1e99fddef\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "\n",
    "def parse_thoughts(response, cur_request_time,max_request_time,debug):\n",
    "    \"\"\"解析模型返回的响应，提取关键信息并格式化为字符串，并根据debug参数决定是否打印\"\"\"\n",
    "    try:\n",
    "        thoughts = response.get(\"thoughts\")\n",
    "        planning = thoughts.get(\"planning\")\n",
    "        reasoning = thoughts.get(\"reasoning\")\n",
    "        reflection = thoughts.get(\"reflection\")\n",
    "        summary = thoughts.get(\"summary\")\n",
    "        observation = response.get(\"observation\")\n",
    "        prompt = f\"planning: {planning}\\nreasoning: {reasoning}\\nreflection: {reflection}\\nobservation: {observation}\\nsummary: {summary}\"\n",
    "        prompt += f\"\\n这是医疗知识库第{cur_request_time}次响应，最多执行{max_request_time}次\\n\" \n",
    "        if debug:\n",
    "            print(prompt)\n",
    "        return prompt\n",
    "    except Exception as e:\n",
    "        print(f\"parse_thoughts error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def execute_action(action_name, action_args, debug):\n",
    "    \"\"\"根据动作名称执行相应的工具函数，并根据debug参数决定是否打印动作信息\"\"\"\n",
    "    try:\n",
    "        func = tools_map.get(action_name)\n",
    "        result = func(**action_args)\n",
    "        if debug:\n",
    "            print(f\"action_name: {action_name}, action_args: {action_args}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"调用工具异常： {e}\")\n",
    "        return str(e)\n",
    "\n",
    "def agent_execute(query, max_request_time, debug=False):\n",
    "    \"\"\"执行代理任务，与模型交互并处理结果，根据debug参数决定是否打印详细信息\"\"\"\n",
    "    cur_request_time = 0\n",
    "    chat_history = []\n",
    "    agent_scratch = \"\"\n",
    "\n",
    "    while cur_request_time < max_request_time:\n",
    "        cur_request_time += 1\n",
    "        prompt = gen_prompt(query, agent_scratch)\n",
    "        response = mp.chat(prompt, chat_history)\n",
    "\n",
    "        if not response or not isinstance(response, dict):\n",
    "            print(f\"call llm exception, response is: {response}\")\n",
    "            continue\n",
    "\n",
    "        action_info = response.get(\"action\")\n",
    "        action_name = action_info.get(\"name\")\n",
    "        action_args = action_info.get(\"args\")\n",
    "        thoughts = response.get(\"thoughts\")\n",
    "\n",
    "        # if debug:\n",
    "        #     print(f'-------------第{cur_request_time}次推断------------')\n",
    "        #     print(f\"observation: {response.get('observation')}\")\n",
    "        #     print(f\"planning: {thoughts.get('planning')}\")\n",
    "        #     print(f\"reasoning: {thoughts.get('reasoning')}\")\n",
    "        #     print(f\"reflection: {thoughts.get('reflection')}\")\n",
    "        #     print(f\"summary: {thoughts.get('summary')}\")\n",
    "        #     print(f\"action_name: {action_name}, action_args: {action_args}\")\n",
    "\n",
    "        if action_name == \"query_user_for_details\":\n",
    "            user_response = input(query_user_for_details(action_args[\"prompt\"]))\n",
    "            chat_history.append([action_args[\"prompt\"], user_response])\n",
    "            agent_scratch += f\"query_user: {action_args['prompt']}user response: {user_response}\"\n",
    "            continue\n",
    "\n",
    "        call_function_result = execute_action(action_name, action_args, debug)\n",
    "        agent_scratch += f\"observation: {response.get('observation')} execute action {action_name} result: {call_function_result}\"\n",
    "        assistant_msg = parse_thoughts(response, cur_request_time,max_request_time,debug)\n",
    "        chat_history.append([user_prompt, assistant_msg])\n",
    "\n",
    "        if action_name == \"finish\":\n",
    "            final_answer = action_args.get(\"answer\")\n",
    "            final_answer = \"\".join([str(key) + str(value) for key, value in final_answer.items()])\n",
    "            break\n",
    "    if cur_request_time == max_request_time:\n",
    "        final_answer=\"本次任务执行失败！,未能检索到相关知识\"\n",
    "    return \"\".join(final_answer)\n",
    "\n",
    "# 示例执行，启用调试模式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import dashscope\n",
    "from prompt import user_prompt\n",
    "from dashscope.api_entities.dashscope_response import Message\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "\n",
    "dashscope.api_key = \"sk-f529539e3a50472fac783cf1e99fddef\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "\n",
    "\n",
    "class ModelProvider(object):\n",
    "    def __init__(self):\n",
    "        # 设置最大重试次数\n",
    "        self.device = \"cuda\"\n",
    "        self.max_retry_time = 1\n",
    "        self.local_run = True\n",
    "        if self.local_run:\n",
    "            # 从环境变量中获取API密钥和模型名称\n",
    "            self.api_key = \"sk-dd3b645e78c24ebbbfc34971555c05fa\"\n",
    "            self.model_name = \"qwen-max\"\n",
    "            # 初始化dashscope客户端\n",
    "            self._client = dashscope.Generation()\n",
    "        else:\n",
    "            # \"Qwen/Qwen1.5-0.5B-chat\" \"Qwen/Qwen2-7B-Instruct\"\n",
    "            self.model_name = \"Qwen/Qwen2-7B-Instruct\"\n",
    "            self.cache_dir = \"/root/autodl-tmp/hug/\"\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                trust_remote_code=True,\n",
    "                cache_dir=self.cache_dir\n",
    "            ).to(self.device)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_name, trust_remote_code=True, cache_dir=self.cache_dir)\n",
    "\n",
    "        \n",
    "    def chat(self, system_prompt, chat_history):\n",
    "        cur_retry_time = 0\n",
    "        while cur_retry_time < self.max_retry_time:\n",
    "            cur_retry_time += 1\n",
    "            if self.local_run:\n",
    "                try:\n",
    "                    # 构建消息列表，包括系统提示、用户提示和历史聊天记录\n",
    "                    messages = [\n",
    "                        Message(role=\"system\", content=system_prompt),\n",
    "                        Message(role=\"user\", content=user_prompt)\n",
    "                    ]\n",
    "                    for his in chat_history:\n",
    "                        messages.append(Message(role=\"user\", content=his[0]))\n",
    "                        messages.append(Message(role=\"assistant\", content=his[1]))\n",
    "                    # 调用模型API并获取响应\n",
    "                    response = self._client.call(\n",
    "                        model=self.model_name,\n",
    "                        api_key=self.api_key,\n",
    "                        messages=messages,\n",
    "                        # result_format='message',  # set the result to be \"message\"  format.\n",
    "                        # stream=True, # set streaming output\n",
    "                        # incremental_output=True  # get streaming output incrementally\n",
    "                        )\n",
    "                    print(response)\n",
    "                    # 解析模型响应并返回内容\n",
    "                    content = self._parse_model_response(response)\n",
    "                    return content\n",
    "                except Exception as e:\n",
    "                    print(f\"call llm exception: {e}\")\n",
    "            else:\n",
    "                try:\n",
    "                    messages = [\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": user_prompt}\n",
    "                        ]\n",
    "                    for his in chat_history:\n",
    "                        messages.append({\"role\": \"user\", \"content\": his[0]})\n",
    "                        messages.append({\"role\": \"assistant\", \"content\": his[1]})\n",
    "                    text = self.tokenizer.apply_chat_template(\n",
    "                        messages,\n",
    "                        tokenize=False,\n",
    "                        add_generation_prompt=True\n",
    "                    )\n",
    "                    model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.device)\n",
    "                    generated_ids = self.model.generate(\n",
    "                        model_inputs.input_ids,\n",
    "                        max_new_tokens=512\n",
    "                    )\n",
    "                    generated_ids = [\n",
    "                        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "                    ]\n",
    "\n",
    "                    response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "                    print(response)\n",
    "                    # 解析模型响应并返回内容\n",
    "                    content = self._parse_model_response(response)\n",
    "                    return content\n",
    "                except Exception as e:\n",
    "                    print(f\"call llm exception: {e}\")        \n",
    "        return {}\n",
    "\n",
    "    def _parse_model_response(self, response):\n",
    "        \"\"\"尝试解析模型响应为JSON格式\"\"\"\n",
    "        text = response[\"output\"][\"text\"] if self.local_run else response\n",
    "        try:\n",
    "            # 尝试直接解析文本为JSON\n",
    "            return json.loads(text)\n",
    "        except json.JSONDecodeError:\n",
    "            # 如果直接解析失败，尝试查找被标记的JSON字符串\n",
    "            json_start = text.find(\"```json\")\n",
    "            json_end = text.rfind(\"```\")\n",
    "            json_content = text[json_start + 7:json_end]\n",
    "            return json.loads(json_content)\n",
    "\n",
    "\n",
    "\n",
    "mp = ModelProvider()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"    你是一个智能知识库助手，专注于提供信息和建议。你可以自主决策，并在处理用户的询问时根据需要调用适当的工具。\n",
    "    目标或其他条件:\n",
    "    腱鞘炎该吃什么药\n",
    "    限制条件说明:\n",
    "    1.仅使用下面列出的动作，特别是在为用户提供医疗建议时，确保所有建议都是基于可用的医学数据和信息。\n",
    "2.你可以根据用户的其他问题自主决策，处理非医疗内容的问题时只需基于已有知识和工具来回答。\n",
    "    动作说明:你可以通过调用以下列出的工具来实现操作，你应尽可能快速、直接地利用已有知识或检索帮助用户解决问题：\n",
    "    1.query_user_for_details:当大模型需要更多具体信息能提供个性化建议或解决方案时，用于向用户提问，以深入了解用户的需求、偏好或限制条件。当query_user不为空时使用该工具, args: [{\"name\": \"prompt\", \"description\": \"向用户提出的问题，旨在澄清或细化其需求。\", \"type\": \"string\"}]\n",
    "2.finish:形成了完整的问答，用户表示满意, args: [{\"name\": \"response\", \"description\": \"在此给出完整的问答信息\", \"type\": \"dict\"}]\n",
    "3.RAG_addition:该模块利用构建的知识图谱，能够提供疾病、症状、治疗方法等之间的详细关系和交互信息。它尤其适合用于理解复杂的医疗关系网络，比如药物副作用或并发症的详细信息。需结合RAG_addition模块提供基础数据。, args: [{\"name\": \"query\", \"description\": \"用户查询的复杂医学条目或概念，涉及系统性知识理解和条件关联的查询。\", \"type\": \"string\"}]\n",
    "4.internet_search_function:通过联网查询获取最新的医学研究成果、临床试验报告及全球卫生政策更新，特别适用于需要最新医学信息的查询。此功能能弥补知识图谱和现有内部数据的不足。, args: [{\"name\": \"query\", \"description\": \"需要了解最新科研成果或最新医学信息的查询，如新型疗法、新药上市信息或新兴病毒的防控措施。\", \"type\": \"string\"}] \n",
    "工具使用逻辑：\n",
    "1. 当用户的问题涉及医学相关知识或健康情况时，首先调用RAG_addition模块。可以提供广泛而深入的信息。\n",
    "2. 如果以上模块提供的信息仍未完全满足用户需求，或者问题涉及最新医学研究成果和全球卫生政策更新，可以启动internet_search_function功能来查找最新医学数据或者别的领域的信息。\n",
    "3. 在用户信息不完整时，使用query_user_for_details工具来询问用户详细情况以获取更多信息。\n",
    "4. 当用户的问题不是关于医学的，请使用大模型自身的能力和internet_search_function联网搜索。\n",
    "\n",
    "    资源说明:\n",
    "    1.提供搜索和信息收集的互联网接入，以便获取最新的医疗或其他相关信息。\n",
    "2.你是一个大语言模型，接受了大量医疗、科技、文化等领域的文本训练，可以帮助解答医疗及其他问题。\n",
    "    策略说明:\n",
    "    1.确保信息准确有效，直接为用户提供最有用的回答。\n",
    "2.基于你检索到的知识或已有的内置知识提供回答，确保及时性和简洁性。\n",
    "3.如果初次使用RAG检索的信息不足。如果涉及到别的领域或者具有时效性的信息，则进行联网搜索，最终如果仍没有匹配，返回'没有检索到合适的答案'。\n",
    "    # agent_scratch:observation: 开始执行查询，以获取关于腱鞘炎治疗药物的详细信息。 execute action RAG_addition result: 腱鞘炎宜食的食物包括有：鸭蛋;鸭翅;鸡爪;鸡蛋\n",
    "推荐食谱包括有：鸡肉蛋花汤;羊肉温补汤;草鱼豆腐;酱肉西兰花;鸡肉炒藕丝;竹筒蒸草鱼;羊肉煎包;鸡肉冬菜饼\n",
    "腱鞘炎通常的使用的药品包括：双氯芬酸钠肠溶片；双氯芬酸二乙胺凝胶；布洛芬片；伤痛酊；布洛芬缓释胶囊；布洛芬缓释混悬液；Ⅰ；酮洛芬凝胶；双氯芬酸钠缓释片；阿西美辛缓释胶囊；依托芬那酯凝胶；复方七叶皂苷钠凝胶\n",
    "    请以json格式进行响应，严格按照响应格式如下:\n",
    "    \n",
    "{\n",
    "    \"action\": {\n",
    "        \"name\": \"动作名称\",\n",
    "        \"args\": {\n",
    "            \"args name\": \"执行动作所需参数的值\"\n",
    "        }\n",
    "    },\n",
    "    \"thoughts\": {\n",
    "        \"planning\": \"解决用户问题的具体实现步骤\",\n",
    "        \"reflection\": \"建设性的自我批评与反思\",\n",
    "        \"summary\": \"当前步骤的总结，用户端接受的是此字段的内容；如果是最后一步finish，在这里给出用户所需要的具体内容\",\n",
    "        \"reasoning\": \"判断信息的合适性，是否需要继续搜索或着提供最终答案，如果信息合适，直接最终答案\"\n",
    "    },\n",
    "    \"observation\": \"观察当前任务的整体进度\"\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = \"\"\n",
    "mp.chat(prompt, chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer,RobertaTokenizer, RobertaModel\n",
    "# from transformers import AutoModel\n",
    "# import torch\n",
    "# from peft import PeftModel\n",
    "# from dashscope.api_entities.dashscope_response import Message\n",
    "# Model_Name = \"Qwen/Qwen1.5-0.5B-chat\"\n",
    "# device = \"cuda\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(Model_Name, trust_remote_code=True,cache_dir=\"/root/autodl-tmp/hug/\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     Model_Name,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     trust_remote_code=True,\n",
    "#     cache_dir=\"/root/autodl-tmp/hug/\"\n",
    "# ).to(device)\n",
    "# # \"Qwen/Qwen2-0.5B\"\n",
    "# # \"Qwen/Qwen1.5-0.5B-chat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel, RobertaTokenizer, RobertaModel\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "# 加载配置\n",
    "with open('/root/autodl-tmp/MedicaAgent/agent/chatbotagent/config.json', encoding='utf-8') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "\n",
    "class RAGModule:\n",
    "    def __init__(self):\n",
    "        # embedding模型\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config['embedding_model_path'])\n",
    "        self.model = AutoModel.from_pretrained(config['embedding_model_path'])\n",
    "        self.index = faiss.read_index(config['vector_dbindex_path'])  # 加载FAISS索引文件\n",
    "        self.top_k = config['rag_retrieval_top_k']                    # 加载top_k\n",
    "\n",
    "        # 加载 roberta-base 模型\n",
    "        self.roberta_tokenizer = RobertaTokenizer.from_pretrained(config['roberta-base_path'])\n",
    "        self.roberta_model = RobertaModel.from_pretrained(config['roberta-base_path'])\n",
    "\n",
    "        # 加载 cross-encoder 模型\n",
    "        self.cross_encoder = CrossEncoder(config['cross-encoder_path'])\n",
    "\n",
    "        # 加载文本块与索引的映射\n",
    "        with open(config['text_mapping_path'], 'r', encoding='utf-8') as f:\n",
    "            self.text_mapping = json.load(f)\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        # 使用 [CLS] token 的输出作为句子向量\n",
    "        return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "    def roberta_encode(self, texts):\n",
    "        inputs = self.roberta_tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            outputs = self.roberta_model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "    def query(self, query_text):\n",
    "        # 使用 FAISS 进行初步召回\n",
    "        query_vector = self.encode_text(query_text)\n",
    "        distances, indices = self.index.search(query_vector, self.top_k)  # 召回 top_k 个文本块\n",
    "\n",
    "        # 获取初步召回的文本块\n",
    "        candidate_texts = [self.text_mapping[str(idx)] for idx in indices[0]]\n",
    "\n",
    "        # 使用 roberta-base 进行高级召回\n",
    "        roberta_query_vector = self.roberta_encode([query_text])[0]\n",
    "        roberta_text_vectors = self.roberta_encode(candidate_texts)\n",
    "        similarities = np.dot(roberta_text_vectors, roberta_query_vector) / (\n",
    "                    np.linalg.norm(roberta_text_vectors, axis=1) * np.linalg.norm(roberta_query_vector))\n",
    "\n",
    "        # 获取与查询文本最相关的 5 个文本块\n",
    "        top_indices = np.argsort(similarities)[-5:]\n",
    "        refined_texts = [candidate_texts[i] for i in top_indices]\n",
    "\n",
    "        # 使用 cross-encoder 进行重排序\n",
    "        cross_encoder_scores = self.cross_encoder.predict([(query_text, text) for text in refined_texts])\n",
    "        ranked_texts = [text for _, text in\n",
    "                        sorted(zip(cross_encoder_scores, refined_texts), key=lambda x: x[0], reverse=True)]\n",
    "\n",
    "        return ranked_texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_module = RAGModule()\n",
    "test_query = \"早上肚子疼\"\n",
    "rag_results_texts = rag_module.query(test_query)\n",
    "print('context', rag_results_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KG 知识图谱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py2neo\n",
    "neo4j_user='neo4j'\n",
    "neo4j_password='neo4jneo4j'\n",
    "neo4j_graph = py2neo.Graph(\"bolt://localhost:7687\", auth=(neo4j_user, neo4j_password))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = \"百日咳\"\n",
    "result_child = neo4j_graph.run(\"MATCH (n {name:'\"+ entity+\"'})-[r]->(m)  \\\n",
    "                RETURN id(r) As id, type(r) As type, \\\n",
    "                n.name As title1 ,m.name As title2,\\\n",
    "                id(n) As id1 ,id(m) As id2,\\\n",
    "                head(labels(n)) As type1, head(labels(m)) As type2\").data()\n",
    "result_parent = neo4j_graph.run(\"MATCH (n)-[r]->(m {name:'\"+ entity+\"'})  \\\n",
    "                                RETURN id(r) As id, type(r) As type, \\\n",
    "                                n.name As title1 ,m.name As title2,\\\n",
    "                                id(n) As id1 ,id(m) As id2,\\\n",
    "                                head(labels(n)) As type1, head(labels(m)) As type2\").data()\n",
    "print(result_child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kg_module import KGmodule\n",
    "kg_model = KGmodule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '腱鞘炎吃什么药？'\n",
    "answer = kg_model.query(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# system_prompt='你是一名医疗知识专家'\n",
    "# user_prompt = \"肚子疼怎么办？\"\n",
    "# # messages = [\n",
    "# #     Message(role=\"system\", content=system_prompt),\n",
    "# #     Message(role=\"user\", content=user_prompt)\n",
    "# # ]\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": system_prompt},\n",
    "#     {\"role\": \"user\", \"content\": user_prompt}\n",
    "# ]\n",
    "# text = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     tokenize=False,\n",
    "#     add_generation_prompt=True\n",
    "# )\n",
    "# model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "# generated_ids = model.generate(\n",
    "#     model_inputs.input_ids,\n",
    "#     max_new_tokens=512\n",
    "# )\n",
    "# generated_ids = [\n",
    "#     output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "# ]\n",
    "\n",
    "# response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_prompt='你是一名医疗知识专家'\n",
    "user_prompt = \"肚子疼怎么办？\"\n",
    "# messages = [\n",
    "#     Message(role=\"system\", content=system_prompt),\n",
    "#     Message(role=\"user\", content=user_prompt)\n",
    "# ]\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "attention_mask = torch.ones(model_inputs.input_ids.shape,device=device)\n",
    "# Generate the response\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=512,\n",
    "    pad_token_id=tokenizer.eos_token_id   # Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
    ")\n",
    "\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 医疗助手的任务描述与限制\n",
    "constraints = [\n",
    "    \"仅使用下面列出的动作，特别是在为用户提供医疗建议时，确保所有建议都是基于可用的医学数据和信息。\",\n",
    "    \"你只能主动行动，这意味着在处理健康咨询时，你需要能够自主决定诊断、治疗建议和健康管理策略。\",\n",
    "    \"你无法与物理对象或药物实体直接交互，因此在推荐药物或治疗时，你必须依赖于医学文献、临床指南和其他权威健康信息。\",\n",
    "    \"你应通过询问用户来了解其详细的健康状况、症状、既往病史和其他相关医疗信息。\"\n",
    "]\n",
    "\n",
    "resources = [\n",
    "    \"提供搜索和信息收集的互联网接入，以便获取最新的医疗信息。\",\n",
    "    \"你是一个大语言模型，接受了大量医疗和健康文本的训练，这将有助于为用户提供详尽的医疗建议，减少对实时数据的依赖。\"\n",
    "]\n",
    "\n",
    "strategies = [\n",
    "    \"不断地回顾和分析你的行为，确保你提供的医疗建议既安全又有效，符合客户的健康需求。\",\n",
    "    \"在提供医疗建议时进行建设性的自我批评，考虑治疗的可行性、患者的特殊状况以及可能影响治疗效果的因素。\",\n",
    "    \"反思你过去的决策和策略，如客户满意度反馈，不断完善你的医疗建议方案。\",\n",
    "    \"每个动作执行都有代价，因此在提供医疗建议时，要平衡成本与收益，确保提供性价比高的选择。\",\n",
    "    \"利用你的信息收集能力来寻找最新、最准确的医疗信息，以增强治疗方案的有效性。\"\n",
    "]\n",
    "\n",
    "# 动作描述\n",
    "actions_description = [\n",
    "    \"RAG模块：当用户询问的健康问题需要更深入的答案时，使用此模块。RAG模块结合检索和生成技术提供综合的答案。\",\n",
    "    \"知识图谱增强模块：在需要详细理解疾病、症状、治疗方法及其相互关系时使用此模块，利用知识图谱提供精准的、上下文相关的信息。\",\n",
    "    \"联网搜索功能：当内置的知识库和图谱不能完全满足信息需求时，使用联网搜索获取最新的医学研究和全球健康信息。\"\n",
    "]\n",
    "\n",
    "# 动作选择逻辑\n",
    "action_logic = \"\"\"\n",
    "1. 当用户的问题涉及需要详尽答案的复杂医学或健康情况时，首先考虑调用RAG模块。这个模块结合了检索和生成策略，可以提供广泛和深入的信息。\n",
    "2. 如果用户的问题更加特定，例如需要了解某种病症的详细治疗方案或药物相互作用，且RAG模块提供的答案未能完全解决用户疑问，则启用知识图谱增强模块。此模块利用构建的医学知识图谱，提供精准和关联性强的信息。\n",
    "3. 当以上两个模块提供的信息仍未能完全满足用户需求，或当问题涉及最新医学研究成果、全球卫生政策更新时，最后考虑使用联网搜索功能。这一步骤可以获取当前最新、最全面的医学数据和信息。\n",
    "\"\"\"\n",
    "\n",
    "# JSON格式响应模板\n",
    "response_format_prompt = \"\"\"\n",
    "{\n",
    "    \"action\": {\n",
    "        \"name\": \"动作名称\",\n",
    "        \"args\": {\n",
    "            \"args name\": \"执行动作所需参数的值\"\n",
    "        }\n",
    "    },\n",
    "    \"thoughts\":{\n",
    "        \"planning\": \"根据用户需求选择最合适的动作执行\",\n",
    "        \"reflection\": \"反思已执行动作的效果，考虑是否需要进一步信息来优化建议\",\n",
    "        \"summary\": \"对当前步骤的总结，以及为用户提供的医疗建议\",\n",
    "        \"reasoning\": \"说明选择特定动作的逻辑，是否需要从用户处获取更多信息\"\n",
    "    },\n",
    "    \"observation\": \"观察整体进展和用户满意度\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# 完整模板调用\n",
    "def generate_prompt(query, agent_scratch):\n",
    "    return prompt_template.format(\n",
    "        query=query,\n",
    "        constraints=\"\\n\".join(constraints),\n",
    "        actions_description=\"\\n\".join(actions_description),\n",
    "        resources=\"\\n\".join(resources),\n",
    "        strategies=\"\\n\".join(strategies),\n",
    "        agent_scratch=agent_scratch,\n",
    "        response_format_prompt=response_format_prompt\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dashscope import Generation\n",
    "from datetime import datetime\n",
    "import random\n",
    "import json\n",
    "\n",
    "\n",
    "# 定义工具列表，模型在选择使用哪个工具时会参考工具的name和description\n",
    "tools = [\n",
    "    # 工具1 获取当前时刻的时间\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_time\",\n",
    "            \"description\": \"当你想知道现在的时间时非常有用。\",\n",
    "            \"parameters\": {}  # 因为获取当前时间无需输入参数，因此parameters为空字典\n",
    "        }\n",
    "    },  \n",
    "    # 工具2 获取指定城市的天气\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"当你想查询指定城市的天气时非常有用。\",\n",
    "            \"parameters\": {  # 查询天气时需要提供位置，因此参数设置为location\n",
    "                        \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"城市或县区，比如北京市、杭州市、余杭区等。\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\n",
    "                \"location\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# 模拟天气查询工具。返回结果示例：“北京今天是晴天。”\n",
    "def get_current_weather(location):\n",
    "    return f\"{location}今天是晴天。 \"\n",
    "\n",
    "# 查询当前时间的工具。返回结果示例：“当前时间：2024-04-15 17:15:18。“\n",
    "def get_current_time():\n",
    "    # 获取当前日期和时间\n",
    "    current_datetime = datetime.now()\n",
    "    # 格式化当前日期和时间\n",
    "    formatted_time = current_datetime.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    # 返回格式化后的当前时间\n",
    "    return f\"当前时间：{formatted_time}。\"\n",
    "\n",
    "# 封装模型响应函数\n",
    "def get_response(messages):\n",
    "    response = Generation.call(\n",
    "        model='qwen-max',\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        seed=random.randint(1, 10000),  # 设置随机数种子seed，如果没有设置，则随机数种子默认为1234\n",
    "        result_format='message',  # 将输出设置为message形式\n",
    "        api_key = \"sk-dd3b645e78c24ebbbfc34971555c05fa\",\n",
    "        model_name = \"qwen-max\"\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def call_with_messages():\n",
    "    print('\\n')\n",
    "    messages = [\n",
    "            {\n",
    "                \"content\": input('请输入：'),  # 提问示例：\"现在几点了？\" \"一个小时后几点\" \"北京天气如何？\"\n",
    "                \"role\": \"user\"\n",
    "            }\n",
    "    ]\n",
    "    \n",
    "    # 模型的第一轮调用\n",
    "    first_response = get_response(messages)\n",
    "    assistant_output = first_response.output.choices[0].message\n",
    "    print(f\"\\n大模型第一轮输出信息：{first_response}\\n\")\n",
    "    messages.append(assistant_output)\n",
    "    if 'tool_calls' not in assistant_output:  # 如果模型判断无需调用工具，则将assistant的回复直接打印出来，无需进行模型的第二轮调用\n",
    "        print(f\"最终答案：{assistant_output.content}\")\n",
    "        return\n",
    "    # 如果模型选择的工具是get_current_weather\n",
    "    elif assistant_output.tool_calls[0]['function']['name'] == 'get_current_weather':\n",
    "        tool_info = {\"name\": \"get_current_weather\", \"role\":\"tool\"}\n",
    "        location = json.loads(assistant_output.tool_calls[0]['function']['arguments'])['properties']['location']\n",
    "        tool_info['content'] = get_current_weather(location)\n",
    "    # 如果模型选择的工具是get_current_time\n",
    "    elif assistant_output.tool_calls[0]['function']['name'] == 'get_current_time':\n",
    "        tool_info = {\"name\": \"get_current_time\", \"role\":\"tool\"}\n",
    "        tool_info['content'] = get_current_time()\n",
    "    print(f\"工具输出信息：{tool_info['content']}\\n\")\n",
    "    messages.append(tool_info)\n",
    "\n",
    "    # 模型的第二轮调用，对工具的输出进行总结\n",
    "    second_response = get_response(messages)\n",
    "    print(f\"大模型第二轮输出信息：{second_response}\\n\")\n",
    "    print(f\"最终答案：{second_response.output.choices[0].message['content']}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    call_with_messages()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
